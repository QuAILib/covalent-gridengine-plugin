{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform FMQA with Covalent and Fixstars Amplify\n",
    "\n",
    "This sample code uses Covalent and Fixstars Amplify to perform FMQA.\n",
    "\n",
    "The code is a modified version of [Amplify Examples](https://github.com/fixstars/amplify-examples/blob/main/notebooks/ja/examples/fmqa_0_algebra.ipynb). Amplify Examples is open source software under the [MIT License](https://github.com/fixstars/amplify-examples/blob/main/LICENSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "### Create SSH Tunnel\n",
    "\n",
    "To connect to the ABCI system via SSH, create an SSH tunnel or `~/.ssh/config` to be able to login using ProxyJump with reference to https://docs.abci.ai/ja/getting-started/#general-method.\n",
    "The sample applications assume that you have created an SSH tunnel using port 10022 on localhost by the following command:\n",
    "\n",
    "```console\n",
    "ssh -i /path/identity_file -L 10022:es:22 -l username as.abci.ai\n",
    "```\n",
    "\n",
    "### Steps to create a virtual environment on the ABCI system\n",
    "\n",
    "Login to the ABCI system and create a virtual environment with the necessary packages installed to perform the tasks in the sample applications.\n",
    "Please refer to [Dependent Packages](#dependencies) for the required packages.\n",
    "For more information on creating a virtual environment on the ABCI system, please refer to https://docs.abci.ai/ja/python/.\n",
    "\n",
    "Below are the steps to create a virtual environment on the ABCI system named `amplify_env`.\n",
    "\n",
    "1. Login to the ABCI system using your own account.\n",
    "2. Run `module load python/3.10/3.10.10` to make python available.\n",
    "3. Create a virtual environment by running `python3 -m venv amplify_env`. The `amplify_env` directory is also created at this time.\n",
    "\n",
    "<a id=\"dependencies\"></a>\n",
    "### Dependent Packages\n",
    "\n",
    "Install the packages required to run the application in your local machine environment and in the virtual environment on the ABCI system.  \n",
    "Each sample application requires the packages listed below.  \n",
    "Bolded packages are required for both the local machine and the virtual environment on the ABCI system, and the rest are required only for the local machine.\n",
    "\n",
    "The sample application uses covalent-gridengine-plugin.  \n",
    "Please refer to the README of covalent-gridengine-plugin for information on how to install and use covalent-gridengine-plugin.\n",
    "\n",
    "- **covalent**\n",
    "- **numpy**\n",
    "- **torch**\n",
    "- **scikit-learn**\n",
    "- amplify<=0.12.1\n",
    "- matplotlib\n",
    "- covalent-gridengine-plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import covalent as ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a GridEngineExecutor object to execute tasks on the ABCI system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "abci_executor = ct.executor.GridEngineExecutor(\n",
    "    username=\"username\",      # Enter your ABCI username.\n",
    "    address=\"localhost\",\n",
    "    port=10022,\n",
    "    ssh_key_file=\"~/.ssh/id_rsa\",  # Enter the path to your ssh key file.\n",
    "    remote_workdir=\"$HOME/amplify_env\",\n",
    "    poll_freq=30,\n",
    "    cleanup=True,\n",
    "    embedded_qsub_args={\n",
    "        \"l\": [\"rt_F=1\", \"h_rt=1:00:00\"],\n",
    "    },  # qsub options to be embedded in the script\n",
    "    qsub_args={\n",
    "        \"g\": \"groupname\",    # Enter your ABCI groupname.\n",
    "    },  # qsub options to be given when it is run on the command line\n",
    "    prerun_commands=[\n",
    "        \"source /etc/profile.d/modules.sh\",\n",
    "        \"module load python/3.10/3.10.10\",\n",
    "        \"module load cuda/11.8/11.8.0\",\n",
    "        \"module load cudnn/8.8/8.8.1\",\n",
    "        \"source ~/amplify_env/bin/activate\",\n",
    "    ],\n",
    "    postrun_commands=[],\n",
    "    bashrc_path=\"~/.bashrc\",\n",
    "    log_stdout=\"stdout.log\",\n",
    "    log_stderr=\"stderr.log\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the necessary functions to run FMQA.\n",
    "\n",
    "The following is an excerpt of code from [Amplify Examples](https://github.com/fixstars/amplify-examples/blob/main/notebooks/ja/examples/fmqa_0_algebra.ipynb) and modified to run using Covalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TorchFM(nn.Module):\n",
    "    def __init__(self, d: int, k: int) -> None:\n",
    "        super().__init__()\n",
    "        self.V = nn.Parameter(torch.randn(d, k), requires_grad=True)\n",
    "        self.lin = nn.Linear(d, 1)  # all coupled network\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out_1 = torch.matmul(x, self.V).pow(2).sum(1, keepdim=True)\n",
    "        out_2 = torch.matmul(x.pow(2), self.V.pow(2)).sum(1, keepdim=True)\n",
    "        out_inter = 0.5 * (out_1 - out_2)\n",
    "        out_lin = self.lin(x)\n",
    "        out = out_inter + out_lin\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import copy\n",
    "\n",
    "# This task is executed on the local machine.\n",
    "# A function that machine learns FM from I/O data\n",
    "@ct.electron(executor=abci_executor)\n",
    "def train(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    model_class: type[torch.nn.Module] | None = None,\n",
    "    model_params: dict | None = None,\n",
    "    batch_size: int = 1024,\n",
    "    epochs: int = 3000,\n",
    "    criterion: torch.nn.Module | None = None,\n",
    "    optimizer_class: type[torch.optim.Optimizer] | None = None,\n",
    "    opt_params: dict | None = None,\n",
    "    lr_sche_class: type[torch.optim.lr_scheduler.LRScheduler] | None = None,\n",
    "    lr_sche_params: dict | None = None,\n",
    ") -> torch.nn.Module:\n",
    "    X_tensor, y_tensor = (\n",
    "        torch.from_numpy(X).float(),\n",
    "        torch.from_numpy(y).float(),\n",
    "    )\n",
    "    indices = np.array(range(X.shape[0]))\n",
    "    indices_train, indices_valid = train_test_split(\n",
    "        indices, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    train_set = TensorDataset(X_tensor[indices_train], y_tensor[indices_train])\n",
    "    valid_set = TensorDataset(X_tensor[indices_valid], y_tensor[indices_valid])\n",
    "    loaders = {\n",
    "        \"train\": DataLoader(train_set, batch_size=batch_size, shuffle=True),\n",
    "        \"valid\": DataLoader(valid_set, batch_size=batch_size, shuffle=False),\n",
    "    }\n",
    "\n",
    "    model = model_class(**model_params)\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    optimizer = optimizer_class(model.parameters(), **opt_params)\n",
    "    if lr_sche_class is not None:\n",
    "        scheduler = lr_sche_class(optimizer, **lr_sche_params)\n",
    "    best_score = 1e18\n",
    "    for epoch in range(epochs):\n",
    "        losses = {\"train\": 0.0, \"valid\": 0.0}\n",
    "\n",
    "        for phase in [\"train\", \"valid\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            for batch_x, batch_y in loaders[phase]:\n",
    "                optimizer.zero_grad()\n",
    "                out = model(batch_x).T[0]\n",
    "                loss = criterion(out, batch_y)\n",
    "                losses[phase] += loss.item() * batch_x.size(0)\n",
    "\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "            losses[phase] /= len(loaders[phase].dataset)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            if best_score > losses[\"valid\"]:\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                best_score = losses[\"valid\"]\n",
    "        if lr_sche_class is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "\n",
    "# This task is executed on ABCI.\n",
    "# A function that evaluates the objective function for input values and creates N0 input-output pairs (initial training data).\n",
    "@ct.electron(executor=abci_executor)\n",
    "def gen_training_data(D: int, N0: int, true_func: Callable) -> tuple[np.ndarray, np.ndarray]:\n",
    "    assert N0 < 2**D\n",
    "\n",
    "    # Generate N0 input values using random numbers.\n",
    "    X = np.random.randint(0, 2, size=(N0, D))\n",
    "\n",
    "    # Exclude duplicate input values from the input values.\n",
    "    # And add the excluded input values using random numbers.\n",
    "    X = np.unique(X, axis=0)\n",
    "    while X.shape[0] != N0:\n",
    "        X = np.vstack((X, np.random.randint(0, 2, size=(N0 - X.shape[0], D))))\n",
    "        X = np.unique(X, axis=0)\n",
    "    y = np.zeros(N0)\n",
    "\n",
    "    # Get the output value corresponding to the N0 input values by evaluating the objective function.\n",
    "    for i in range(N0):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Generating {i}-th training data set.\")\n",
    "        y[i] = true_func(X[i])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the sample code below is implemented with amplify v0.12 and is not guaranteed to work with amplify v1.0.0 or later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import amplify\n",
    "\n",
    "# Defining FM as QUBO from FM parameters.\n",
    "# As with the previously defined TorchFM class, the formula is written as g(x).\n",
    "def FM_as_QUBO(x: np.ndarray, w0: np.ndarray, w: np.ndarray, v: np.ndarray, k: int) -> amplify.BinaryPoly:\n",
    "    lin = w0 + (x.T @ w)\n",
    "    D = w.shape[0]\n",
    "    out_1 = amplify.sum_poly(k, lambda i: amplify.sum_poly(D, lambda j: x[j] * v[j, i]) ** 2)\n",
    "\n",
    "    # Note that x[j] is equivalent x[j] * x[j] because x[j] is a binary variable.\n",
    "    out_2 = amplify.sum_poly(\n",
    "        k, lambda i: amplify.sum_poly(D, lambda j: x[j] * v[j, i] * v[j, i])\n",
    "    )\n",
    "    return lin + (out_1 - out_2) / 2\n",
    "\n",
    "# This task is executed on the local machine.\n",
    "# A function that performs a solution using a Ising machine.\n",
    "@ct.electron\n",
    "def step(model: amplify.BinaryPoly, D: int, k: int) -> np.ndarray:\n",
    "    client = amplify.client.FixstarsClient()\n",
    "    client.parameters.timeout = 1000\n",
    "    client.token = \"xxxxxxxxxxxxxxx\"  # Enter your token of Amplify AE.\n",
    "    solver = amplify.Solver(client)\n",
    "\n",
    "    v, w, w0 = list(model.parameters())\n",
    "    v = v.detach().numpy()\n",
    "    w = w.detach().numpy()[0]\n",
    "    w0 = w0.detach().numpy()[0]\n",
    "\n",
    "    gen = amplify.BinarySymbolGenerator()\n",
    "    q = gen.array(D)\n",
    "    cost = FM_as_QUBO(q, w0, w, v, k)  # Defining FM as QUBO from FM parameters.\n",
    "    result = solver.solve(cost)  # Solve QUBO using Ising machine.\n",
    "    if len(result.solutions) == 0:\n",
    "        raise RuntimeError(\"No solution was found.\")\n",
    "    values = result.solutions[0].values\n",
    "    q_values = q.decode(values)\n",
    "    return q_values\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# A function that plots the history of objective function evaluation values for the initial training data and the i-th FMQA cycle.\n",
    "def plot_history(y: np.ndarray, N: int, N0: int) -> plt.Figure:\n",
    "    assert y is not None\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    plt.plot(\n",
    "        [i for i in range(N0)],\n",
    "        y[: N0],\n",
    "        marker=\"o\",\n",
    "        linestyle=\"-\",\n",
    "        color=\"b\",\n",
    "    )  # Objective function evaluation value for the initial training data (random process)\n",
    "\n",
    "    plt.plot(\n",
    "        [i for i in range(N0, N)],\n",
    "        y[N0 :],\n",
    "        marker=\"o\",\n",
    "        linestyle=\"-\",\n",
    "        color=\"r\",\n",
    "    )  # Objective function evaluation value for the i-th FMQA cycle.\n",
    "    plt.xlabel(\"i-th evaluation of f(x)\", fontsize=18)\n",
    "    plt.ylabel(\"f(x)\", fontsize=18)\n",
    "    plt.tick_params(labelsize=18)\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a d-dimensional symmetric matrix with zero mean of the components.\n",
    "def make_Q(d: int) -> np.ndarray:\n",
    "    Q_true = np.random.rand(d, d)\n",
    "    Q_true = (Q_true + Q_true.T) / 2\n",
    "    Q_true = Q_true - np.mean(Q_true)\n",
    "    return Q_true\n",
    "\n",
    "def get_true_func(D: int) -> Callable:\n",
    "    Q = make_Q(D)\n",
    "    def true_func(x):\n",
    "        return x @ Q @ x\n",
    "    return true_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This task is executed on ABCI.\n",
    "@ct.electron(executor=abci_executor)\n",
    "def append_new_data(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    x_hat: np.ndarray,\n",
    "    pred_x: np.ndarray,\n",
    "    pred_y: np.ndarray,\n",
    "    D: int,\n",
    "    true_func: Callable,\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    # If the same input value already exists in the training data as x_hat, the surrounding values are used as x_hat.\n",
    "    is_identical = True\n",
    "    while is_identical:\n",
    "        is_identical = False\n",
    "        for j in range(X.shape[0]):\n",
    "            if np.all(x_hat == X[j, :]):\n",
    "                change_id = np.random.randint(0, D, 1)\n",
    "                x_hat[change_id.item()] = 1 - x_hat[change_id.item()]\n",
    "                is_identical = True\n",
    "                break\n",
    "    y_hat = true_func(x_hat)\n",
    "\n",
    "    # Add the input-output pair [x_hat, y_hat] at the optimal point to the training data.\n",
    "    X = np.vstack((X, x_hat))\n",
    "    y = np.append(y, y_hat)\n",
    "\n",
    "    # If the objective function evaluation value is updated to the minimum value, copy the input-output pair to [pred_x, pred_y].\n",
    "    if pred_y > y_hat:\n",
    "        pred_y = y_hat\n",
    "        pred_x = x_hat\n",
    "        print(f\"variable updated, {pred_y=}\")\n",
    "    else:\n",
    "        print(\"\")\n",
    "    return pred_x, pred_y, X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the above 4 `electron` to create a `lattice` workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.lattice\n",
    "def workflow(D: int, N: int, N0: int, k: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    true_func = get_true_func(D)\n",
    "    X, y = gen_training_data(D, N0, true_func)\n",
    "    print(f\"Starting FMQA cycles...\")\n",
    "    pred_x = X[0]\n",
    "    pred_y = 1e18\n",
    "    for i in range(N - N0):\n",
    "        print(f\"FMQA Cycle #{i} \", end=\"\")\n",
    "        model = train(X, y,\n",
    "            model_class=TorchFM,\n",
    "            model_params={\"d\": D, \"k\": k},\n",
    "            batch_size=8,\n",
    "            epochs=2000,\n",
    "            criterion=nn.MSELoss(),\n",
    "            optimizer_class=torch.optim.AdamW,\n",
    "            opt_params={\"lr\": 1},)\n",
    "        x_hat = step(model, D, k)\n",
    "        pred_x, pred_y, X, y = append_new_data(X, y, x_hat, pred_x, pred_y, D, true_func)\n",
    "\n",
    "        # If all inputs have been fully searched, exit the for loop.\n",
    "        #if len(y) >= 2**D:\n",
    "            #print(f\"Fully searched. Terminating FMQA cycles.\")\n",
    "            #return None\n",
    "\n",
    "    return pred_x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input dimension (problem size)\n",
    "D = 100\n",
    "N = 70  # Number of times the function can be evaluated\n",
    "N0 = 60  # Number of initial training data samples\n",
    "k = 10  # Dimension of the vector in FM (hyper parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, start the covalent server with the following command\n",
    "\n",
    "```console\n",
    "covalent start\n",
    "````\n",
    "\n",
    "With the default configuration, you can view the Covalent GUI by accessing http://localhost:48008/ from your browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, execute the workflow. First, dispatch the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispatch_id = ct.dispatch(workflow)(D, N, N0, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dispatch_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ct.get_result(dispatch_id, wait=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the history of the objective function evaluation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_x, y = result.result\n",
    "\n",
    "fig = plot_history(y, N, N0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covalent-gridengine-plugin-f7KjlutS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
